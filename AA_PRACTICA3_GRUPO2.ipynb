{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PaulaDiz/APRENDIZAJE_AUTOMATICO_23_24/blob/main/AA_PRACTICA3_GRUPO2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#GRUPO B2\n",
        "Rubén Sierra Serrano, Paula Diz Diz y Lucía de Angulo Pelayo."
      ],
      "metadata": {
        "id": "FFWQ5YftV5aQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ENUNCIADO PRÁCTICA 3: Crea un modelo de Red de Neuronas Convolucionales que sea capaz de reconocer y clasificar imágenes en 100 categorías. Este modelo será definido, configurado, entrenado, evaluado y mejorado para posteriormente usarlo para hacer predicciones."
      ],
      "metadata": {
        "id": "BVndPJ2tY2LK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Primero importamos las librerías necesarias para ejecutar nuestro modelo:"
      ],
      "metadata": {
        "id": "mtWpg0C7ZAuT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.datasets import cifar100\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "LW39z2OGITmB"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##CARGA DE LOS DATOS\n",
        "Descargamos el dataset y cargamos los datos de entrenamiento y de test."
      ],
      "metadata": {
        "id": "upUNu2vkZVag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = cifar100.load_data(label_mode = 'coarse')\n",
        "\n",
        "x_train = x_train.astype('float32')/255\n",
        "x_test = x_test.astype('float32')/255\n",
        "\n",
        "\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)"
      ],
      "metadata": {
        "id": "qf-37nlR9IZN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comprobación de los tamaños de los datos de entrenamiento y test: y = etiquetas; x = imagenes . Como podemos observar se van a usar 50 mil imágenes para training, que son las imágenes con las que se entrenará nuestro modelo, y 10 mil imágenes de test.  Después de haber entrenado el modelo con las imágenes de training, crearemos una red de neuronas capaz de recononcer imágenes que nunca había visto antes, es decir, nuestras imagenes de test."
      ],
      "metadata": {
        "id": "KP2luMJ_Zkio"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Forma de x_train:\", len(x_train), \" Forma de y_train:\",len(y_train))\n",
        "print(\"Forma de x_test:\", len(x_test), \" Forma de y_test:\", len(y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOLflxRUaJxz",
        "outputId": "d9210f32-f6db-4068-be3e-43b42f5f7f65"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forma de x_train: 50000  Forma de y_train: 50000\n",
            "Forma de x_test: 10000  Forma de y_test: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos a visualizar alguna imagen del set de entrenamiento."
      ],
      "metadata": {
        "id": "LVLbVxDAc5EM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.imshow(x_train[3])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "aMuudKZQa1U4",
        "outputId": "e9dcae93-e4ce-49f4-ccfa-5fdd4a87c3b2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x78399ed508b0>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como vemos, las imágenes de CIFAR-100 tienen baja calidad, por lo que la complicación de trabajar con ellas es mayor a otras bases de datos que contienen imágenes nítidas."
      ],
      "metadata": {
        "id": "oaYvqEsPdCMk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##LIMPIEZA Y MEJORA DE LA BASE DE DATOS\n",
        "\n",
        "La calidad de la base de datos tiene un impacto directo en la calidad del modelo. Una base de datos de alta calidad ofrecerá datos de entrenamiento precisos y representativos, lo que resulta en un modelo más generalizable y correcto. Los datos de entrenamiento deben representar de manera adecuada la variabilidad y la diversidad de todos los datos para que el modelo pueda aprender patrones útiles. Limpiar la base de datos nos ayudará a construir modelos que pueden manejar datos con ruido, outliers y variaciones en la entrada. La diversidad en los datos permite que el modelo se adapte a diferentes escenarios y condiciones."
      ],
      "metadata": {
        "id": "cu5d1pxJafvV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Buscamos y eliminamos las imágenes duplicadas:"
      ],
      "metadata": {
        "id": "BJx-mvbgbgLy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "#convertimos imágenes para que sean unidimensionales\n",
        "x_train_flat = x_train.reshape(x_train.shape[0], -1)\n",
        "x_test_flat = x_test.reshape(x_test.shape[0], -1)\n",
        "\n",
        "#comprobar si hay imágenes duplicadas en el set de entrenamiento\n",
        "duplicates_train = len(x_train_flat) - len(np.unique(x_train_flat, axis=0))\n",
        "if duplicates_train > 0:\n",
        "    print(f\"Hay {duplicates_train} imágenes duplicadas en el conjunto de entrenamiento.\")\n",
        "else:\n",
        "    print(\"No hay imágenes duplicadas en el conjunto de entrenamiento.\")\n",
        "\n",
        "#ahora comprobamos en el set de test\n",
        "duplicates_test = len(x_test_flat) - len(np.unique(x_test_flat, axis=0))\n",
        "if duplicates_test > 0:\n",
        "    print(f\"Hay {duplicates_test} imágenes duplicadas en el conjunto de test.\")\n",
        "else:\n",
        "    print(\"No hay imágenes duplicadas en el conjunto de test.\")\n",
        "\n",
        "#ahora buscamos los índices de las imágenes replicadas. primero las de train\n",
        "unique_indices_train = np.unique(x_train_flat, axis=0, return_index=True)[1]\n",
        "duplicated_indices_train = np.setdiff1d(np.arange(x_train.shape[0]), unique_indices_train)\n",
        "\n",
        "#lo mismo pero en las de test\n",
        "unique_indices_test = np.unique(x_test_flat, axis=0, return_index=True)[1]\n",
        "duplicated_indices_test = np.setdiff1d(np.arange(x_test.shape[0]), unique_indices_test)\n",
        "\n",
        "#Ahora eliminamos las imágenes duplicadas\n",
        "x_train_filtered = np.delete(x_train, duplicated_indices_train, axis=0)\n",
        "y_train_filtered = np.delete(y_train, duplicated_indices_train, axis=0)\n",
        "duplicates_after_removal = len(x_train) - len(x_train_filtered)\n",
        "if duplicates_after_removal > 0:\n",
        "    print(f\"Se han eliminado {duplicates_after_removal} imágenes duplicadas del conjunto de entrenamiento.\")\n",
        "else:\n",
        "    print(\"No hay imágenes duplicadas en el conjunto de entrenamiento.\")\n",
        "\n",
        "x_test_filtered = np.delete(x_test, duplicated_indices_test, axis=0)\n",
        "y_test_filtered = np.delete(y_test, duplicated_indices_test, axis=0)\n",
        "\n",
        "#comprobar cuántas imágenes se han eliminado\n",
        "duplicates_after_removal = len(x_test) - len(x_test_filtered)\n",
        "if duplicates_after_removal > 0:\n",
        "    print(f\"Se eliminaron {duplicates_after_removal} imágenes duplicadas del conjunto de test.\")\n",
        "else:\n",
        "    print(\"No hay imágenes duplicadas en el conjunto de test.\")"
      ],
      "metadata": {
        "id": "Y_2QdzS8bpX-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verificamos si hay etiquetas 'NaN':"
      ],
      "metadata": {
        "id": "djQrHK9Id-O_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Verificar si hay NaN en las etiquetas de entrenamiento\n",
        "nan_train_labels = np.isnan(y_train).any()\n",
        "\n",
        "# Verificar si hay NaN en las etiquetas de test\n",
        "nan_test_labels = np.isnan(y_test).any()\n",
        "\n",
        "if nan_train_labels:\n",
        "    print(\"Se han encontrado valores NaN en las etiquetas de entrenamiento.\")\n",
        "else:\n",
        "    print(\"No se han encontrado valores NaN en las etiquetas de entrenamiento.\")\n",
        "\n",
        "if nan_test_labels:\n",
        "    print(\"Se han encontrado valores NaN en las etiquetas de test.\")\n",
        "else:\n",
        "    print(\"No se han encontrado valores NaN en las etiquetas de test.\")"
      ],
      "metadata": {
        "id": "MRlX6IRXeMVP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48f34ece-6a81-4e70-e1bf-dcaeba848c6d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No se han encontrado valores NaN en las etiquetas de entrenamiento.\n",
            "No se han encontrado valores NaN en las etiquetas de test.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##CONSTRUIMOS LA RED"
      ],
      "metadata": {
        "id": "yRrg15iPefKw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3,3), activation='relu', input_shape=(32, 32, 3)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(256, (3,3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(128, (3,3), activation='relu'))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(100, activation='softmax'))\n",
        "\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "6gCj_Exr9MyP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8011b3e0-fcad-4f69-bd07-07d4065368cc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 30, 30, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 15, 15, 32)        0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 13, 13, 256)       73984     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 6, 6, 256)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 4, 4, 128)         295040    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 4, 4, 128)         0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 2048)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                131136    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 100)               6500      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 507556 (1.94 MB)\n",
            "Trainable params: 507556 (1.94 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este resumen nos ofrece información sobre la arquitectura y los parámetros de la CNN.  \n",
        "Se trata de un modelo secuencial, es decir, las capas van una tras otra.  La arquitectura de la red consta de 9 capas: las tres capas convolucionales (Conv2D) tienen 32, 256, 128 neuronas respectivamente y filtros de tamaño (3,3).  Hay dos capas MaxPooling2D que se emplean para reducir la resolución espacial de las representaciones de características.  Ayudan a disminuir el número de parámetros y los cálculos que realiza la red.\n",
        "\n",
        "En cuanto a los parámetros, en total la red tiene 507556 entrenables."
      ],
      "metadata": {
        "id": "cO9rgm6xh-JM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tras probar diversas combinaciones de hiperparámetros y estructuras de la red, hemos llegado a esta, la cual nos ha dado el mejor resultado (55% de precisión).  Parece ser eficiente el poner dos capas convencionales al final de la estructura (capas Dense).  El input shape es (32, 32, 3) porque las imágenes son de 32x32 y son a color.  La capa final contiene 100 neuronas porque hay 100 cateogorías en las cuales son clasificadas las imágenes y la función de activación empleada en dicha capa es 'softmax', la cual es ampliamente utilizada en problemas de clasificación multiclase al dar salida a valores entre 0 y 1 (probabilidades de pertenencia a cierta clase).  "
      ],
      "metadata": {
        "id": "_ZqXQKk9eo0f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos a normalizar los datos para que los valores de las matrices de las imágenes se encuentren en el rango [0, 1].  Las etiquetas las pasamos a one-hot encoding."
      ],
      "metadata": {
        "id": "3MkhNSvKgMMB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-u-kMnFBgCey"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compilamos el programa.\n",
        "El optimizador 'adam' resulta muy eficiente para los modelos de redes neuronales convolucionales.  Empleamos 'categorical_crossentropy' para calcular la pérdida porque nuestras etiquetas están en formato one-hot."
      ],
      "metadata": {
        "id": "DAu_TRRwfwHR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "dRzywBmC9RqQ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entrenamiento del modelo."
      ],
      "metadata": {
        "id": "zQjTOFkCj0ZJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora elegimos el 20% de los datos de entrenamiento para que sean de validación y así poder ir comprobando que nuestro modelo entrena correctamente.  Acto seguido entrenamos el modelo."
      ],
      "metadata": {
        "id": "NKjadUIqjW9f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2)\n",
        "\n",
        "history = model.fit(x_train, y_train, epochs=30,batch_size=64, validation_data=(x_val,y_val))"
      ],
      "metadata": {
        "id": "rvV9pHLy9Tmo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 771
        },
        "outputId": "ed9b2878-2b56-4a99-9a4f-5d263b10f3f3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-c3f659ae0137>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1377, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1360, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1349, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1127, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1185, in compute_loss\n        return self.compiled_loss(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/compile_utils.py\", line 277, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 143, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 270, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 2221, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend.py\", line 5575, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (64, 20) and (64, 100) are incompatible\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comprobamos la validez de nuestro modelo observando la precisión que este presenta con los datos de test, que son datos que nunca ha visto."
      ],
      "metadata": {
        "id": "480zFydNj48L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)\n",
        "print(f'\\nAccuracy en el conjunto de prueba: {test_acc}')"
      ],
      "metadata": {
        "id": "CaJyeAMX9Xqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# APORTACIONES E INVESTIGACIÓN EXTRA\n",
        "### EfficientNet\n",
        "En este proyecto experimentamos con diversas arquitecturas de redes neuronales, como la familia de las EfficientNet, ResNet, DenseNet y VGG16. Estas arquitecturas, debido a su complejidad interna, el agotamiento de los recursos disponibles en Google Colab y el tiempode ejecución, limitaron nuestras pruebas. Perp observamos que las redes ResNet resultaron ser altamente efectivas.\n",
        "\n",
        "## Optimizador SAM\n",
        "También hemos encontrado que SAM es un enfoque de optimización para entrenar modelos de aprendizaje profundo. La idea principal detrás de SAM es mejorar la generalización del modelo minimizando la sensibilidad del modelo a pequeñas perturbaciones en los datos de entrada. En lugar de simplemente minimizar la pérdida en los datos de entrenamiento, SAM también considera la \"sharpness\" de la función de pérdida. La nitidez se refiere a cuán abruptamente cambia la pérdida en las cercanías del punto actual en el espacio de parámetros del modelo. SAM introduce un término de regularización que penaliza los cambios bruscos en los parámetros del modelo, lo que ayuda a suavizar la función de pérdida y mejorar la generalización.\n",
        "\n",
        "## Co-Learning\n",
        "También hemos explorado un método conocido como Co-Learning, en el cual dos o más redes neuronales aprenden de manera colaborativa. En nuestro, nos hemos enfrentado a la limitación de Colab, ya que requiere la ejecución simultánea de varias redes neuronales.\n",
        "\n",
        "## Fractional maxpooling\n",
        "Además probamos con CNNs que emplean fractional maxpooling, que ayuda a reducir el tamaño de la información y proporciona cierta invariancia a cambios en la posición y forma de las imágenes al aplicar pooling, lo que permite ajustar la cantidad de reducción de datos de manera más flexible. Esta mejora resulta en un rendimiento mejorado en la tarea de reconocimiento de patrones en conjuntos de datos específicos.\n",
        "\n",
        "## Transfer learning\n",
        "Otro factor interesante que hemos estudiado es el transfer learning, o 'aprendizaje por transferencia'. Es un enfoque en el aprendizaje automático en el cual se utiliza el conocimiento adquirido al resolver una tarea para mejorar el rendimiento en otra tarea relacionada. En lugar de entrenar un modelo desde cero para cada tarea específica, se aprovechan los conocimientos previos aprendidos en una tarea para ayudar en el aprendizaje de otra tarea.\n",
        "\n",
        "## Transformaciones de imágenes para la mejora de la base de datos.\n",
        "Al neceesitar unos buenos datos hemos intentado realizar distintas transformaciones a los datos, para que estos sean mejores y faciliten el entrenamiento, debido a la capacidad de RAM esto no ha sido posible, ya que ocupaba gran cantidad de RAM.\n",
        "\n",
        "### Aumento de imágenes\n",
        "Aumentar la resolución de las imágenes proporciona más detalles y contexto a la red neuronal durante el entrenamiento. Esto puede ayudar a mejorar la capacidad de la red para generalizar patrones y características a diferentes escalas.\n",
        "\n",
        "CODIGO \"lo implantamos comentado por si pudiera ocasionar algún error en la compilación del programa\"\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jQjonFGDlT4v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import numpy as np\n",
        "#import matplotlib.pyplot as plt\n",
        "#from scipy.ndimage import zoom\n",
        "\n",
        "\n",
        "#import numpy as np\n",
        "\n",
        "#función para aplicar el aumento a todas las imágenes de la base de datos\n",
        "\n",
        "#from scipy.ndimage import zoom\n",
        "#def aumentar_imagen(imagen, factor_aumento=2):\n",
        "    #imagen_aumentada = zoom(imagen, (factor_aumento, factor_aumento, 1), order=2)\n",
        "    #return imagen_aumentada\n",
        "\n",
        "#Uso de la función para el aumento\n",
        "\n",
        "#imagen_aumentada_train = list(map(lambda imagen: aumentar_imagen(imagen, 2), x_train))\n",
        "#imagen_aumentada_test = list(map(lambda imagen: aumentar_imagen(imagen, 2), x_test))\n",
        "\n",
        "# Visualizamos y comparamos algunas imágenes\n",
        "#for i in range(5):\n",
        "  #plt.figure(figsize=(10, 10))\n",
        "\n",
        "  #plt.subplot(1, 2, 1)\n",
        "  #plt.imshow(imagen_aumentada_train[i])\n",
        "  #plt.title(\"Aumentada\")\n",
        "  #plt.axis(\"off\")\n",
        "  #plt.imshow(imagen_aumentada_train[i], cmap='gray')\n",
        "\n",
        "  #plt.subplot(1, 2, 2)\n",
        "  #plt.imshow(x_train[i])\n",
        "  #plt.title(\"Original\")\n",
        "  #plt.axis(\"off\")\n",
        "  #plt.imshow(x_train[i], cmap='gray')\n",
        "\n",
        "  #plt.show()"
      ],
      "metadata": {
        "id": "qnNmOKQrnNF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Brillo y contraste\n",
        "Al igual que el aumento de imágenes, exponer la red neuronal a imágenes con diferentes niveles de brillo y contraste, se mejora su capacidad para adaptarse a nuevas condiciones de iluminación en el mundo real.\n",
        "\n",
        "CODIGO \"lo implantamos comentado por si pudiera ocasionar algún error en la compilación del programa\""
      ],
      "metadata": {
        "id": "QgRcYeovo68R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#intalación de la librería para poder ejecutar el código\n",
        "#pip install Pillow"
      ],
      "metadata": {
        "id": "ZGuEilpBp5RA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from PIL import Image, ImageEnhance\n",
        "#import os\n",
        "#import numpy as np\n",
        "\n",
        "#función para ajustar los brillos\n",
        "\n",
        "#def ajustes_brillo_contraste(input_folder, output_folder, brillo_factor=1.5, contraste_factor=1.5):\n",
        "    # Asegúrate de que la carpeta de salida exista\n",
        "#    if not os.path.exists(output_folder):\n",
        " #       os.makedirs(output_folder)\n",
        "#\n",
        "    # Iterar sobre todas las imágenes en la carpeta de entrada\n",
        " #   for i, imagen_array in enumerate(x_train):\n",
        "        # Convertir el array de la imagen a un objeto Image de Pillow\n",
        "  #      imagen = Image.fromarray(imagen_array.astype('uint8'))  # Convertir el tipo de datos a uint8\n",
        "\n",
        "        # Aplicar ajustes aleatorios de brillo y contraste\n",
        "   #     imagen = ajustar_brillo_contraste(imagen, brillo_factor, contraste_factor)\n",
        "\n",
        "        # Crear la ruta de salida y guardar la nueva imagen\n",
        "    #    output_path = os.path.join(output_folder, f\"imagen_{i}.png\")\n",
        "     #   imagen.save(output_path)\n",
        "\n",
        "#función parfa ajustar las imágenes a los brillos\n",
        "\n",
        "#def ajustar_brillo_contraste(imagen, brillo_factor, contraste_factor):\n",
        " #   enhancer = ImageEnhance.Brightness(imagen)\n",
        "  #  imagen = enhancer.enhance(np.random.uniform(1, brillo_factor))\n",
        "\n",
        "   # enhancer = ImageEnhance.Contrast(imagen)\n",
        "\n",
        "    #imagen = enhancer.enhance(np.random.uniform(1, contraste_factor))\n",
        "\n",
        "    #return imagen\n",
        "\n",
        "#input_folder = \"imagen_aumentada_train\"\n",
        "#output_folder = \"imagen_aumentada_train\"\n",
        "#brillo_factor = 1.5\n",
        "#contraste_factor = 1.5\n",
        "\n",
        "#ajustes_brillo_contraste(input_folder, output_folder, brillo_factor, contraste_factor)\n",
        "\n",
        "#mostramos las imágenes para ver la diferencia\n",
        "\n",
        "#plt.figure(figsize=(20, 20))\n",
        "#for i in range(5):\n",
        "    # Ploteamos sin máscara\n",
        "    #plt.subplot(2, 5, 1 + i)\n",
        "    #plt.axis('off')\n",
        "    #plt.imshow(imagen_aumentada_train[i], cmap='gray')\n",
        "\n",
        "#plt.show()"
      ],
      "metadata": {
        "id": "6Y5SckVBpbFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Máscara Sharpen\n",
        "Puede resaltar detalles finos y mejorar la definición de los objetos en las imágenes.\n",
        "\n",
        "CODIGO \"lo implantamos comentado por si pudiera ocasionar algún error en la compilación del programa\""
      ],
      "metadata": {
        "id": "qZgGVR9-rhzY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import numpy as np\n",
        "#import cv2\n",
        "\n",
        "#función para aplicar la máscara\n",
        "\n",
        "#def sharpen(dataset):\n",
        "    #kernel = (-1 / 256.0) * np.array(\n",
        "        #np.asarray([[1, 4, 6, 4, 1], [4, 16, 24, 16, 4], [6, 24, -476, 24, 6], [4, 16, 24, 16, 4], [1, 4, 6, 4, 1]]))\n",
        "\n",
        "    #for i in range(len(dataset)):\n",
        "        #dataset[i] = cv2.filter2D(np.array(dataset[i]), -1, kernel)  # Convertir la lista a un arreglo usando np.array()\n",
        "\n",
        "    #return dataset\n",
        "\n",
        "#x_train_filtered = sharpen(imagen_aumentada_train)\n",
        "\n",
        "#Visualizar imágenes\n",
        "\n",
        "#for i in range(5):\n",
        "  #plt.figure(figsize=(8, 8))\n",
        "\n",
        "  #plt.subplot(1, 2, 1)\n",
        "  #plt.imshow(imagen_aumentada_train[i])\n",
        "  #plt.title(\"sin mascara\")\n",
        "  #plt.axis(\"off\")\n",
        "  #plt.imshow(imagen_aumentada_train[i], cmap='gray')\n",
        "\n",
        "  #plt.subplot(1, 2, 2)\n",
        "  #plt.imshow(x_train[i])\n",
        "  #plt.title(\"con mascara\")\n",
        "  #plt.axis(\"off\")\n",
        "  #plt.imshow(x_train_filtered[i], cmap='gray')\n",
        "\n",
        "  #plt.show()"
      ],
      "metadata": {
        "id": "MZ5ieCwzr9yd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}